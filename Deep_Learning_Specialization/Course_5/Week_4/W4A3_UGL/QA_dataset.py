# # Transformer Network Application: Question Answering
# ## 1 - Extractive Question Answering
# ### 1.1 - Data preprocessing
from datasets import load_from_disk

# Load a dataset and print the first example in the training set
babi_dataset = load_from_disk('data/')
print(babi_dataset['train'][0])


# Take a look at the format of the data. For a given story, there are two sentences which serve as the context,
# and one question. Each of these phrases has an ID. There is also a supporting fact ID which refers to a sentence
# in the story that helps answer the question. For example, for the question 'What is east of the hallway?',
# the supporting fact 'The bedroom is east of the hallway' has the ID '2'.
# There is also the answer, 'bedroom' for the question.
babi_dataset['train'][102]

# Check and see if the entire dataset of stories has this format.
type_set = set()
for story in babi_dataset['train']:
    if str(story['story']['type'] )not in type_set:
        type_set.add(str(story['story']['type'] ))

type_set


# To make the data easier to work with, you will flatten the dataset to transform it from a dictionary
# structure to a table structure.
flattened_babi = babi_dataset.flatten()

flattened_babi

next(iter(flattened_babi['train']))


# Now it is much easier to access the information you need! You can now easily extract the answer, question,
# and facts from the story, and also join the facts into a single entry under 'sentences'.
def get_question_and_facts(story):
    dic = {}
    dic['question'] = story['story.text'][2]
    dic['sentences'] = ' '.join([story['story.text'][0], story['story.text'][1]])
    dic['answer'] = story['story.answer'][2]
    return dic


processed = flattened_babi.map(get_question_and_facts)

processed['train'][2]

processed['test'][2]


# The goal of extractive QA is to find the part of the text that contains the answer to the question.
# You will identify the position of the answer using the indexes of the string. For example,
# if the answer to some question was 'September', you would need to find the start and end string indices of the word
# 'September' in the context sentence 'Jane visits Africa in September.'
# Use this next function to get the start and end indices of the answer in each of the stories in your dataset.

def get_start_end_idx(story):
    str_idx = story['sentences'].find(story['answer'])
    end_idx = str_idx + len(story['answer'])
    return {'str_idx':str_idx,
          'end_idx': end_idx}


processed = processed.map(get_start_end_idx)

num = 187
print(processed['test'][num])
start_idx = processed['test'][num]['str_idx']
end_idx = processed['test'][num]['end_idx']
print('answer:', processed['test'][num]['sentences'][start_idx:end_idx])


# ### 1.2 - Tokenize and Align with ðŸ¤— Library
# 
# Now you have all the data you need to train a Transformer model to perform Question Answering!
# You are ready for a task you may have already encountered in the Named-Entity Recognition lab - tokenizing
# and aligning your input. To feed text data to a Transformer model, you will need to tokenize your input using a
# [ðŸ¤— Transformer tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html). It is crucial that
# the tokenizer you use must match the Transformer model type you are using! In this exercise, you will use the ðŸ¤—
# [DistilBERT fast tokenizer](https://huggingface.co/transformers/model_doc/distilbert.html), which standardizes
# the length of your sequence to 512 and pads with zeros.

# Transformer models are often trained by tokenizers that split words into subwords. For instance, the word
# 'Africa' might get split into multiple subtokens. This can create some misalignment between the list of tags for
# the dataset and the list of labels generated by the tokenizer, since the tokenizer can split one word into several,
# or add special tokens. Before processing, it is important that you align the start and end indices with the tokens
# associated with the target answer word with a `tokenize_and_align()` function. In this case, since you are interested
# in the start and end indices of the answer, you will want to align the index of the sentence to match the index
# of the token for a word.

from transformers import DistilBertTokenizerFast
tokenizer = DistilBertTokenizerFast.from_pretrained('tokenizer/')

def tokenize_align(example):
    encoding = tokenizer(example['sentences'], example['question'], truncation=True, padding=True, max_length=tokenizer.model_max_length)
    start_positions = encoding.char_to_token(example['str_idx'])
    end_positions = encoding.char_to_token(example['end_idx']-1)
    if start_positions is None:
        start_positions = tokenizer.model_max_length
    if end_positions is None:
        end_positions = tokenizer.model_max_length
    return {'input_ids': encoding['input_ids'],
          'attention_mask': encoding['attention_mask'],
          'start_positions': start_positions,
          'end_positions': end_positions}

qa_dataset = processed.map(tokenize_align)

qa_dataset = qa_dataset.remove_columns(['story.answer', 'story.id', 'story.supporting_ids', 'story.text', 'story.type'])

qa_dataset['train'][200]


# - The goal of *extractive* QA is to identify the portion of the text that contains the answer to a question.
# - Transformer models are often trained by tokenizers that split words into subwords.
#   - Before processing, it is important that you align the start and end indices with the tokens associated
#   with the target answer word.

# # 2 - Training 
# Now that you have finished tokenizing and aligning your data, you can feed it into a pre-trained ðŸ¤—
# Transformer model! You will use a DistilBERT model, which matches the tokenizer you used to preprocess your data.

train_ds = qa_dataset['train']
test_ds = qa_dataset['test']

from transformers import TFDistilBertForQuestionAnswering
model = TFDistilBertForQuestionAnswering.from_pretrained("model/tensorflow", return_dict=False)

# ### 2.1 - TensorFlow implementation
# For this assignment you will execute two implemenations, one in TensorFlow and one in PyTorch.
# #### Train and test datasets
# * In the TensorFlow implementation, you will have to set the data format type to tensors,
# which may create ragged tensors (tensors of different lengths).
# * You will have to convert the ragged tensors to normal tensors using the `to_tensor()` method, which pads
# the tensors and sets the dimensions to `[None, tokenizer.model_max_length]` so you can feed different size
# tensors into your model based on the batch size.

import tensorflow as tf

columns_to_return = ['input_ids','attention_mask', 'start_positions', 'end_positions']

train_ds.set_format(type='tf', columns=columns_to_return)

train_features = {x: train_ds[x] for x in ['input_ids', 'attention_mask']}
train_labels = {"start_positions": tf.reshape(train_ds['start_positions'], shape=[-1,1]),
                'end_positions': tf.reshape(train_ds['end_positions'], shape=[-1,1])}

train_tfdataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels)).batch(8)

# #### Training
# It is finally time to start training your model!
# * Create a custom training function using
# [tf.GradientTape()](https://www.tensorflow.org/api_docs/python/tf/GradientTape)
# * Target two loss functions, one for the start index and one for the end index. 
# * `tf.GradientTape()` records the operations performed during forward prop for automatic
# differentiation during backprop.

EPOCHS = 10
loss_fn1 = tf.keras.losses.SparseCategoricalCrossentropy( from_logits=True)
loss_fn2 = tf.keras.losses.SparseCategoricalCrossentropy( from_logits=True)
opt = tf.keras.optimizers.Adam(learning_rate=3e-5)

losses = []
for epoch in range(EPOCHS):
    print("Starting epoch: %d"% epoch )
    for step, (x_batch_train, y_batch_train) in enumerate(train_tfdataset):
        with tf.GradientTape() as tape:
            answer_start_scores, answer_end_scores = model(x_batch_train)
            loss_start = loss_fn1(y_batch_train['start_positions'], answer_start_scores)
            loss_end = loss_fn2(y_batch_train['end_positions'], answer_end_scores)
            loss = 0.5 * (loss_start + loss_end)
        losses.append(loss)
        grads = tape.gradient(loss, model.trainable_weights)
        opt.apply_gradients(zip(grads, model.trainable_weights))

        if step % 20 == 0:
            print("Training loss (for one batch) at step %d: %.4f"% (step, 
                                                                   float(loss_start)))


# Take a look at your losses and try playing around with some of the hyperparameters for better results!
import matplotlib.pyplot as plt

plt.plot(losses)
plt.show()

# You have successfully trained your model to help automatically answer
# questions! Try asking it a question about a story.

question, text = 'What is south of the bedroom?','The hallway is south of the garden. The garden is south of the bedroom.'
input_dict = tokenizer(text, question, return_tensors='tf')
outputs = model(input_dict)
start_logits = outputs[0]
end_logits = outputs[1]

all_tokens = tokenizer.convert_ids_to_tokens(input_dict["input_ids"].numpy()[0])
answer = ' '.join(all_tokens[tf.math.argmax(start_logits, 1)[0] : tf.math.argmax(end_logits, 1)[0]+1])
print(question, answer.capitalize())


# ## 2.2 PyTorch implementation
# 
# [PyTorch](https://pytorch.org/) is an open source machine learning framework developed by Facebook's
# AI Research lab that can be used for computer vision and natural language processing. As you can imagine,
# it is quite compatible with the bAbI dataset.

# #### Train and test dataset
# Go ahead and try creating a train and test dataset by importing PyTorch.

from torch.utils.data import DataLoader

columns_to_return = ['input_ids', 'attention_mask', 'start_positions', 'end_positions']
train_ds.set_format(type='pt', columns=columns_to_return)
test_ds.set_format(type='pt', columns=columns_to_return)


# For the accuracy metrics for the PyTorch implementation, you will change things up a bit and use the
# [F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) for start
# and end indicies over the entire test dataset as the loss functions.

from sklearn.metrics import f1_score


def compute_metrics(pred):
    start_labels = pred.label_ids[0]
    start_preds = pred.predictions[0].argmax(-1)
    end_labels = pred.label_ids[1]
    end_preds = pred.predictions[1].argmax(-1)
    
    f1_start = f1_score(start_labels, start_preds, average='macro')
    f1_end = f1_score(end_labels, end_preds, average='macro')
    
    return {
        'f1_start': f1_start,
        'f1_end': f1_end,
    }


# #### Training
# Now it is time to load a pre-trained model.
# **Note:** You will be using the DistilBERT instead of TFDistilBERT for a PyTorch implementation.

del model # We delete the tensorflow model to avoid memory issues

from transformers import DistilBertForQuestionAnswering

pytorch_model = DistilBertForQuestionAnswering.from_pretrained("model/pytorch")

# Instead of a custom training loop, you will use the
# [ðŸ¤— Trainer](https://huggingface.co/transformers/main_classes/trainer.html), which contains a basic training
# loop and is fairly easy to implement in PyTorch.

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir='results',          # output directory
    overwrite_output_dir=True,
    num_train_epochs=3,              # total number of training epochs
    per_device_train_batch_size=8,  # batch size per device during training
    per_device_eval_batch_size=8,   # batch size for evaluation
    warmup_steps=20,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir=None,            # directory for storing logs
    logging_steps=50
)

trainer = Trainer(
    model=pytorch_model,                 # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_ds,         # training dataset
    eval_dataset=test_ds,
    compute_metrics=compute_metrics             # evaluation dataset
)

trainer.train()
trainer.evaluate(test_ds)


# Now it is time to ask your PyTorch model a question! 
# * Before testing your model with a question, you can tell PyTorch to send your model and inputs to the GPU if
# your machine has one, or the CPU if it does not.
# * You can then proceed to tokenize your input and create PyTorch tensors and send them to your device. 
# * The rest of the pipeline is relatively similar to the one you implemented for TensorFlow.

import torch

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

pytorch_model.to(device)

question, text = 'What is east of the hallway?','The kitchen is east of the hallway. The garden is south of the bedroom.'

input_dict = tokenizer(text, question, return_tensors='pt')

input_ids = input_dict['input_ids'].to(device)
attention_mask = input_dict['attention_mask'].to(device)

outputs = pytorch_model(input_ids, attention_mask=attention_mask)

start_logits = outputs[0]
end_logits = outputs[1]

all_tokens = tokenizer.convert_ids_to_tokens(input_dict["input_ids"].numpy()[0])
answer = ' '.join(all_tokens[torch.argmax(start_logits, 1)[0] : torch.argmax(end_logits, 1)[0]+1])

print(question, answer.capitalize())
