# # Transformer Network Application: Named-Entity Recognition
import pandas as pd
import numpy as np
import tensorflow as tf
import json
import random
import logging
import re

tf.get_logger().setLevel('ERROR')


# ## 1 - Named-Entity Recogniton to Process Resumes
# ### 1.1 - Dataset Cleaning
df_data = pd.read_json("ner.json", lines=True)
df_data = df_data.drop(['extras'], axis=1)
df_data['content'] = df_data['content'].str.replace("\n", " ")

df_data.head()

df_data.iloc[0]['annotation']

def mergeIntervals(intervals):
    sorted_by_lower_bound = sorted(intervals, key=lambda tup: tup[0])
    merged = []

    for higher in sorted_by_lower_bound:
        if not merged:
            merged.append(higher)
        else:
            lower = merged[-1]
            if higher[0] <= lower[1]:
                if lower[2] is higher[2]:
                    upper_bound = max(lower[1], higher[1])
                    merged[-1] = (lower[0], upper_bound, lower[2])
                else:
                    if lower[1] > higher[1]:
                        merged[-1] = lower
                    else:
                        merged[-1] = (lower[0], higher[1], higher[2])
            else:
                merged.append(higher)
    return merged


def get_entities(df):
    
    entities = []
    
    for i in range(len(df)):
        entity = []
    
        for annot in df['annotation'][i]:
            try:
                ent = annot['label'][0]
                start = annot['points'][0]['start']
                end = annot['points'][0]['end'] + 1
                entity.append((start, end, ent))
            except:
                pass
    
        entity = mergeIntervals(entity)
        entities.append(entity)
    
    return entities


df_data['entities'] = get_entities(df_data)
df_data.head()


def convert_dataturks_to_spacy(dataturks_JSON_FilePath):
    try:
        training_data = []
        lines=[]
        with open(dataturks_JSON_FilePath, 'r') as f:
            lines = f.readlines()

        for line in lines:
            data = json.loads(line)
            text = data['content'].replace("\n", " ")
            entities = []
            data_annotations = data['annotation']
            if data_annotations is not None:
                for annotation in data_annotations:
                    #only a single point in text annotation.
                    point = annotation['points'][0]
                    labels = annotation['label']
                    # handle both list of labels or a single label.
                    if not isinstance(labels, list):
                        labels = [labels]

                    for label in labels:
                        point_start = point['start']
                        point_end = point['end']
                        point_text = point['text']
                        
                        lstrip_diff = len(point_text) - len(point_text.lstrip())
                        rstrip_diff = len(point_text) - len(point_text.rstrip())
                        if lstrip_diff != 0:
                            point_start = point_start + lstrip_diff
                        if rstrip_diff != 0:
                            point_end = point_end - rstrip_diff
                        entities.append((point_start, point_end + 1 , label))
            training_data.append((text, {"entities" : entities}))
        return training_data
    except Exception as e:
        logging.exception("Unable to process " + dataturks_JSON_FilePath + "\n" + "error = " + str(e))
        return None

def trim_entity_spans(data: list) -> list:
    """Removes leading and trailing white spaces from entity spans.

    Args:
        data (list): The data to be cleaned in spaCy JSON format.

    Returns:
        list: The cleaned data.
    """
    invalid_span_tokens = re.compile(r'\s')

    cleaned_data = []
    for text, annotations in data:
        entities = annotations['entities']
        valid_entities = []
        for start, end, label in entities:
            valid_start = start
            valid_end = end
            while valid_start < len(text) and invalid_span_tokens.match(
                    text[valid_start]):
                valid_start += 1
            while valid_end > 1 and invalid_span_tokens.match(
                    text[valid_end - 1]):
                valid_end -= 1
            valid_entities.append([valid_start, valid_end, label])
        cleaned_data.append([text, {'entities': valid_entities}])
    return cleaned_data  


data = trim_entity_spans(convert_dataturks_to_spacy("ner.json"))


from tqdm.notebook import tqdm
def clean_dataset(data):
    cleanedDF = pd.DataFrame(columns=["setences_cleaned"])
    sum1 = 0
    for i in tqdm(range(len(data))):
        start = 0
        emptyList = ["Empty"] * len(data[i][0].split())
        numberOfWords = 0
        lenOfString = len(data[i][0])
        strData = data[i][0]
        strDictData = data[i][1]
        lastIndexOfSpace = strData.rfind(' ')
        for i in range(lenOfString):
            if (strData[i]==" " and strData[i+1]!=" "):
                for k,v in strDictData.items():
                    for j in range(len(v)):
                        entList = v[len(v)-j-1]
                        if (start>=int(entList[0]) and i<=int(entList[1])):
                            emptyList[numberOfWords] = entList[2]
                            break
                        else:
                            continue
                start = i + 1  
                numberOfWords += 1
            if (i == lastIndexOfSpace):
                for j in range(len(v)):
                        entList = v[len(v)-j-1]
                        if (lastIndexOfSpace>=int(entList[0]) and lenOfString<=int(entList[1])):
                            emptyList[numberOfWords] = entList[2]
                            numberOfWords += 1
        cleanedDF = cleanedDF.append(pd.Series([emptyList],  index=cleanedDF.columns ), ignore_index=True )
        sum1 = sum1 + numberOfWords
    return cleanedDF


cleanedDF = clean_dataset(data)

# Take a look at your cleaned dataset and the categories the named-entities are matched to, or 'tags'.
cleanedDF.head()

# ### 1.2 - Padding and Generating Tags
# Now, it is time to generate a list of unique tags you will match the named-entities to.
unique_tags = set(cleanedDF['setences_cleaned'].explode().unique())#pd.unique(cleanedDF['setences_cleaned'])#set(tag for doc in cleanedDF['setences_cleaned'].values.tolist() for tag in doc)
tag2id = {tag: id for id, tag in enumerate(unique_tags)}
id2tag = {id: tag for tag, id in tag2id.items()}

unique_tags


# Next, you will create an array of tags from your cleaned dataset. Oftentimes, your input sequence can exceeds
# the maximum length of a sequence your network can process, so it needs to be cut off to that desired maximum length.
# And when the input sequence is shorter than the desired length, you need to append zeroes onto its end using this
# [Keras padding API](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences).

from tensorflow.keras.preprocessing.sequence import pad_sequences

MAX_LEN = 512
labels = cleanedDF['setences_cleaned'].values.tolist()

sequences = [[tag2id.get(l) for l in lab] for lab in labels]

tags = pad_sequences(sequences, maxlen=MAX_LEN, 
                     value=tag2id["Empty"], padding="post",
                     dtype="long", truncating="post")
tags


# ### 1.3 - Tokenize and Align Labels with ðŸ¤— Library
# Before feeding the texts to a Transformer model, you will need to tokenize your input using a
# [ðŸ¤— Transformer tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html).
# It is crucial that the tokenizer you use must match the Transformer model type you are using!
# In this exercise, you will use the ðŸ¤—
# [DistilBERT fast tokenizer](https://huggingface.co/transformers/model_doc/distilbert.html),
# which standardizes the length of your sequence to 512 and pads with zeros. Notice this matches
# the maximum length you used when creating tags.

gpus = tf.config.list_physical_devices('GPU')
if gpus:
    for gpu in gpus:
        tf.config.experimental.set_virtual_device_configuration(gpu,[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])


from transformers import DistilBertTokenizerFast #, TFDistilBertModel
tokenizer = DistilBertTokenizerFast.from_pretrained('tokenizer/')


# Transformer models are often trained by tokenizers that split words into subwords.
# For instance, the word 'Africa' might get split into multiple subtokens. This can create some
# misalignment between the list of tags for the dataset and the list of labels generated by the tokenizer,
# since the tokenizer can split one word into several, or add special tokens. Before processing, it is important
# that you align the lists of tags and the list of labels generated by the selected tokenizer with
# a `tokenize_and_align_labels()` function.

# ### Exercise 1 - tokenize_and_align_labels
# Implement `tokenize_and_align_labels()`. The function should perform the following:
# * The tokenizer cuts sequences that exceed the maximum size allowed by your model with the parameter `truncation=True`
# * Aligns the list of tags and labels with the tokenizer `word_ids` method returns a list that maps the subtokens to
# the original word in the sentence and special tokens to `None`.
# * Set the labels of all the special tokens (`None`) to -100 to prevent them from affecting the loss function. 
# * Label of the first subtoken of a word and set the label for the following subtokens to -100. 

label_all_tokens = True


def tokenize_and_align_labels(tokenizer, examples, tags):
    tokenized_inputs = tokenizer(examples, truncation=True, is_split_into_words=False, padding='max_length', max_length=512)
    labels = []
    for i, label in enumerate(tags):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            # Special tokens have a word id that is None. We set the label to -100 so they are automatically
            # ignored in the loss function.
            if word_idx is None:
                label_ids.append(-100)
            # We set the label for the first token of each word.
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            # For the other tokens in a word, we set the label to either the current label or -100, depending on
            # the label_all_tokens flag.
            else:
                label_ids.append(label[word_idx] if label_all_tokens else -100)
            previous_word_idx = word_idx

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs


# Now that you have tokenized inputs, you can create train and test datasets!
test = tokenize_and_align_labels(tokenizer, df_data['content'].values.tolist(), tags)
train_dataset = tf.data.Dataset.from_tensor_slices((
    test['input_ids'],
    test['labels']
))

# ### 1.4 - Optimization
# Fantastic! Now you can finally feed your data into into a pretrained ðŸ¤— model.
# You will optimize a DistilBERT model, which matches the tokenizer you used to preprocess your data.
# Try playing around with the different hyperparamters to improve your results!
from transformers import TFDistilBertForTokenClassification

model = TFDistilBertForTokenClassification.from_pretrained('model/', num_labels=len(unique_tags))

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)
model.compile(optimizer=optimizer, loss=model.hf_compute_loss, metrics=['accuracy']) # can also use any keras loss fn
model.fit(train_dataset.batch(4),
          epochs=10, 
          batch_size=4)


text = "Manisha Bharti. 3.5 years of professional IT experience in Banking and Finance domain"
inputs = tokenizer(text, return_tensors="tf", truncation=True, is_split_into_words=False, padding="max_length", max_length=512 )
input_ids = inputs["input_ids"]
#inputs["labels"] = tf.reshape(tf.constant([1] * tf.size(input_ids).numpy()), (-1, tf.size(input_ids)))

output = model(inputs).logits
prediction = np.argmax(output, axis=2)
print(prediction)

model(inputs)

pred_labels = []

get_ipython().system('pip install seqeval')

true_labels = [[id2tag.get(true_index, "Empty") for true_index in test['labels'][i]] for i in range(len(test['labels']))]
np.array(true_labels).shape

output = model.predict(train_dataset)

predictions = np.argmax(output['logits'].reshape(220, -1, 12), axis=-1)

predictions.shape

from matplotlib import pyplot as plt 

p = plt.hist(np.array(true_labels).flatten())
plt.xticks(rotation='vertical')
plt.show()

from collections import Counter
Counter(np.array(true_labels).flatten())

pred_labels = [[id2tag.get(index, "Empty") for index in predictions[i]] for i in range(len(predictions))]
p = plt.hist(np.array(pred_labels).flatten())
plt.xticks(rotation='vertical')
plt.show()

from seqeval.metrics import classification_report
print(classification_report(true_labels, pred_labels))
